{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYng21dfDeLn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFzPM1MEDWm9"
   },
   "outputs": [],
   "source": [
    "# Download file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7jyLleI2D4Mc",
    "outputId": "64e6e76f-125b-4d06-ff66-235f2d20ac8c"
   },
   "outputs": [],
   "source": [
    "# Process file (Pandas)\n",
    "file = open(path_to_file)\n",
    "file_string = file.read()\n",
    "file_translations = [translation.split('\\t') for translation in file_string.split('\\n')]\n",
    "df = pd.DataFrame(file_translations)\n",
    "df = df.drop(df.index[118964]) # error with None in last line\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1gUkCeRMJYq"
   },
   "outputs": [],
   "source": [
    "# IN: array of sentences\n",
    "# OUT: array of numbers - sentences (sentences, words), tokenizer\n",
    "def tokenize(sentences):\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "  tokenizer.fit_on_texts(sentences)\n",
    "  \n",
    "  sentences = tokenizer.texts_to_sequences(sentences)\n",
    "  sentences = tf.keras.preprocessing.sequence.pad_sequences(sentences, padding='post')\n",
    "  return sentences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUrZ2PGbP_gi"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = unicode_to_ascii(sentence.lower().strip())\n",
    "\n",
    "  # space between word and punctuation after it\n",
    "  sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "  \n",
    "  sentence = sentence.strip()\n",
    "  return '<start> ' + sentence + ' <end>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UzCOVsdpHaK1",
    "outputId": "7efdf845-135f-494b-835c-ae372b17e56b"
   },
   "outputs": [],
   "source": [
    "# test train split\n",
    "eng_sentences = [preprocess_sentence(sentence) for sentence in df[0]]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in df[1]]\n",
    "\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(eng_sentences, spa_sentences, test_size=0.2) # input, output\n",
    "\n",
    "input_train, eng_tokenizer = tokenize(input_train)\n",
    "output_train, spa_tokenizer = tokenize(output_train)\n",
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "2ayFb63ILuKK",
    "outputId": "b861e9a4-3c7e-4009-8d52-b819c978909f"
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "EPOCHS = 10\n",
    "BUFFER_SIZE = len(input_train)\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCHS = BUFFER_SIZE//BATCH_SIZE\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "VOCAB_INPUT_SIZE = len(eng_tokenizer.word_index) + 1\n",
    "VOCAB_OUTPUT_SIZE = len(spa_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, output_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "example_input, example_output = next(iter(dataset))\n",
    "WORDS_INPUT = len(example_input[1])\n",
    "WORDS_OUTPUT = len(example_output[1])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNf1wkA79yhV"
   },
   "source": [
    "# Encoder, Decoder, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipHhA0TGLkMC"
   },
   "outputs": [],
   "source": [
    "# INPUT:\n",
    "# -- (BATCH_SIZE, WORDS) array of sentences\n",
    "# -- (BATCH_SIZE, UNITS) enc_hidden\n",
    "# OUTPUT:\n",
    "# -- (BATCH_SIZE, WORDS, UNITS) x\n",
    "# -- (BATCH_SIZE, UNITS) enc_hidden\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, VOCAB_INPUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(VOCAB_INPUT_SIZE, EMBEDDING_DIM)\n",
    "    self.gru = tf.keras.layers.GRU(UNITS, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "  \n",
    "  def call(self, x, enc_hidden):\n",
    "    x = self.embedding(x)\n",
    "    x, enc_hidden = self.gru(x, initial_state=enc_hidden)\n",
    "    return x, enc_hidden\n",
    "  \n",
    "  def initialize_enc_hidden(self):\n",
    "    return tf.zeros((BATCH_SIZE, UNITS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qya_epfoAueJ"
   },
   "outputs": [],
   "source": [
    "# INPUT:\n",
    "# -- (BATCH_SIZE, WORDS, UNITS) x\n",
    "# -- (BATCH_SIZE, UNITS) enc_hidden\n",
    "# OUTPUT\n",
    "# -- (BATCH_SIZE, WORDS, 1) attention_weights\n",
    "# -- (BATCH_SIZE, UNITS) context_vector\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, UNITS):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.D1 = tf.keras.layers.Dense(UNITS)\n",
    "    self.D2 = tf.keras.layers.Dense(UNITS)\n",
    "    self.D3 = tf.keras.layers.Dense(1)\n",
    "  \n",
    "  def call(self, x, enc_hidden):\n",
    "    x = self.D1(x)\n",
    "    enc_hidden = self.D2(tf.expand_dims(enc_hidden, axis=1))\n",
    "    attention_weights = self.D3(tf.nn.tanh(x + enc_hidden))\n",
    "    attention_weights = tf.nn.softmax(attention_weights)\n",
    "    context_vector = attention_weights * x\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORKnTLKtFA6V"
   },
   "outputs": [],
   "source": [
    "# INPUT\n",
    "# -- (BATCH_SIZE, 1) x\n",
    "# -- (BATCH_SIZE, WORDS, UNITS) enc_output\n",
    "# -- (BATCH_SIZE, UNITS) enc_hidden\n",
    "# OUTPUT\n",
    "# -- (BATCH_SIZE, VOCAB_OUTPUT_SIZE) x\n",
    "# -- (BATCH_SIZE, UNITS) hidden\n",
    "# -- (BATCH_SIZE, WORDS, 1) attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, UNITS, VOCAB_OUTPUT_SIZE, EMBEDDING_DIM, BATCH_SIZE):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.BATCH_SIZE = BATCH_SIZE\n",
    "    self.attention = BahdanauAttention(UNITS)\n",
    "    self.embedding = tf.keras.layers.Embedding(VOCAB_OUTPUT_SIZE, EMBEDDING_DIM)\n",
    "    self.gru = tf.keras.layers.GRU(UNITS, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    self.D1 = tf.keras.layers.Dense(VOCAB_OUTPUT_SIZE, activation='softmax')\n",
    "  \n",
    "  def call(self, x, enc_output, hidden):\n",
    "    context_vector, attention_weights = self.attention(enc_output, hidden)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    x, hidden = self.gru(x)\n",
    "    x = tf.reshape(x, [self.BATCH_SIZE, -1])\n",
    "    x = self.D1(x)\n",
    "    return x, hidden, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uobn36I6daws"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "rAFoAj4ydLlA",
    "outputId": "5c836827-acfb-4ff7-916a-d7114908a763"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpPKWpX6OEkI"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))  # False for all padded sequences\n",
    "  loss_ = loss_obj(real, pred)                     # Loss (64)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)             # Mask (64) - 0 for all 0\n",
    "  loss_ *= mask                                       # Change all loss for paddings to 0\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKb4dit0dJhO"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(VOCAB_INPUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "decoder = Decoder(UNITS, VOCAB_OUTPUT_SIZE, EMBEDDING_DIM, BATCH_SIZE)\n",
    "\n",
    "checkpoint_dir = \"/content/drive/My Drive/code/checkpoints\"\n",
    "checkpoint_prefix = f\"{checkpoint_dir}/ckpt\"\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "# checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XD_2Bo2LwFQ"
   },
   "outputs": [],
   "source": [
    "def train_step(input, target, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(input, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    \n",
    "    for t in range(1, target.shape[1]):\n",
    "      predictions, dec_hidden, attention_weights = decoder(dec_input, enc_output, dec_hidden)\n",
    "      loss += loss_function(target[:,t], predictions)\n",
    "      dec_input = tf.expand_dims(target[:,t], 1)\n",
    "      pass\n",
    "    pass\n",
    "  batch_loss = loss / int(target.shape[1])\n",
    "  \n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "AjaDCST1VJxM",
    "outputId": "b9d9bfd8-dcb6-4017-f095-f4951961bc0e"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  enc_hidden = encoder.initialize_enc_hidden()\n",
    "  total_loss = 0\n",
    "  for batch, (input, target) in enumerate(dataset.take(STEPS_PER_EPOCHS)):\n",
    "    batch_loss = train_step(input, target, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    if batch % 100 == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "      pass\n",
    "    pass\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    pass\n",
    "  print(f'Epoch {epoch + 1} Loss {total_loss.numpy():.4f}')\n",
    "  print(f'Time taken for 1 epoch {time.time() - start} sec\\n')\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "IxA5ux0dXipJ",
    "outputId": "4702a8f6-bed0-469d-96c2-5af74cb5e6bd"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "  sentence = [eng_tokenizer.word_index[word] for word in sentence.split()]\n",
    "  sentence = tf.keras.preprocessing.sequence.pad_sequences([sentence], maxlen=WORDS_INPUT, padding='post')\n",
    "  sentence = tf.convert_to_tensor(sentence)\n",
    "\n",
    "  result = ''\n",
    "  enc_hidden = tf.zeros((1, UNITS))\n",
    "  enc_output, enc_hidden = encoder(sentence, enc_hidden)\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([spa_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "  for i in range(WORDS_OUTPUT):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input, enc_output, dec_hidden)\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    result += spa_tokenizer.index_word[predicted_id] + ' '\n",
    "    if spa_tokenizer.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pri0Htidcv4F"
   },
   "outputs": [],
   "source": [
    "result, sentence = translate('I am finding a job')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Language Translation (2)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
